# -*- coding: utf-8 -*-
"""ResNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NFsbHq_AwxGFmeQpPRht5PDviqc9UIrH
"""

from google.colab import drive
drive.mount('/content/drive')
images_root =  '/content/drive/MyDrive/HVD_dataset/all_openrooms/main_xml'
labels_root =  '/content/drive/MyDrive/HVD_dataset/all_openrooms/labels_main_xml'

import os
import re
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms
from torchvision.models import resnet18

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

def create_image_label_mapping(images_root, labels_root, max_samples=750):
    mapping = []
    for folder in os.listdir(images_root):
        img_folder = os.path.join(images_root, folder)
        label_folder = os.path.join(labels_root, folder)
        if not os.path.isdir(img_folder) or not os.path.isdir(label_folder):
            continue
        for fname in os.listdir(img_folder):
            if fname.endswith(".png") and fname.startswith("im_"):
                match = re.match(r'im_(\d+)\.png', fname)
                if match:
                    img_id = match.group(1)
                    img_path = os.path.join(img_folder, fname)
                    label_path = os.path.join(label_folder, f"imsemLabel_{img_id}.npy")
                    if os.path.exists(label_path):
                        mapping.append((img_path, label_path))
                        if len(mapping) >= max_samples:
                            return mapping
    return mapping

# This function matches each input image with its corresponding label mask (.npy) and stops when it reaches max_samples

class ImageLabelDataset(Dataset):
    def __init__(self, mapping):
        self.mapping = mapping
        self.image_transform = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.ToTensor(),
        ])
        self.label_resize = transforms.Resize((64, 64), interpolation=transforms.InterpolationMode.NEAREST)

    def __len__(self):
        return len(self.mapping)

    def __getitem__(self, idx):
        img_path, label_path = self.mapping[idx]
        image = Image.open(img_path).convert("RGB")
        label = np.load(label_path)

        label = Image.fromarray(label.astype(np.uint8))
        label = self.label_resize(label)
        label = torch.from_numpy(np.array(label)).long()

        return self.image_transform(image), label

# Loads a (image, label) pair, resizes both to 64×64 and converts image to tensor and label to integer tensor

class ResNet18Segmentation(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        # Load pretrained ResNet-18 and remove the classifier
        resnet = resnet18(pretrained=True)
        self.encoder = nn.Sequential(*list(resnet.children())[:-2])  # up to conv5

        # Decoder = upsample step-by-step to get back to original spatial resolution
        self.decoder = nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 7x7 → 14x14
            nn.Conv2d(256, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 14x14 → 28x28
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),  # 28x28 → 56x56
            nn.Conv2d(64, num_classes, 1),  # 1x1 conv to get class scores
            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)  # 56x56 → 224x224
        )

    def forward(self, x):
        x = self.encoder(x) # extract feature maps
        x = self.decoder(x) # upsample to match input size
        return x  # shape: [B, num_classes, H, W]

#Predict a class label for each pixel by turning a high-level feature map into a full-size segmentation map.

def train_model(model, train_loader, val_loader, epochs=10):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    train_losses = []
    train_accuracies = []
    val_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        correct_train = 0
        total_train_pixels = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_train_loss += loss.item()

            preds = torch.argmax(outputs, dim=1)
            correct_train += (preds == labels).sum().item()
            total_train_pixels += labels.numel()

        train_loss = total_train_loss / len(train_loader)
        train_acc = 100 * correct_train / total_train_pixels

        # Validation
        model.eval()
        total_val_loss = 0
        correct_val = 0
        total_val_pixels = 0

        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)

                total_val_loss += loss.item()

                preds = torch.argmax(outputs, dim=1)
                correct_val += (preds == labels).sum().item()
                total_val_pixels += labels.numel()

        val_loss = total_val_loss / len(val_loader)
        val_acc = 100 * correct_val / total_val_pixels

        # Save metrics
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | "
              f"Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")

    return train_losses, train_accuracies, val_losses, val_accuracies

#Trains and evaluates the model over multiple epochs, tracking loss and pixel-wise accuracy.

def detect_num_classes(mapping, max_scan=100):
    unique_labels = set()
    for _, label_path in mapping[:max_scan]:
        label = np.load(label_path)
        unique_labels.update(np.unique(label).tolist())
    num_classes = max(unique_labels) + 1  # assume 0-based classes
    print(f"Detected {num_classes} classes: {sorted(unique_labels)}")
    return num_classes

images_root =  '/content/drive/MyDrive/HVD_dataset/all_openrooms/main_xml'
labels_root = '/content/drive/MyDrive/HVD_dataset/all_openrooms/labels_main_xml'
mapping = create_image_label_mapping(images_root, labels_root)
print(f"Found {len(mapping)} samples")

num_classes = detect_num_classes(mapping)
model = ResNet18Segmentation(num_classes=num_classes).to(device)

import matplotlib.pyplot as plt

def plot_training_and_validation(train_losses, train_accs, val_losses, val_accs):
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(12, 5))

    # Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss", marker='o')
    plt.plot(epochs, val_losses, label="Val Loss", marker='o')
    plt.title("Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    # Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accs, label="Train Accuracy", marker='o')
    plt.plot(epochs, val_accs, label="Val Accuracy", marker='o')
    plt.title("Pixel Accuracy over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()

    plt.tight_layout()
    plt.show()

def get_class_ids_from_dataset(mapping):
    all_class_ids = set()
    for _, label_path in mapping:
        label = np.load(label_path)
        all_class_ids.update(np.unique(label).tolist())
    return sorted(all_class_ids)

class_ids = get_class_ids_from_dataset(mapping)
print("Class IDs found:", class_ids)

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap
import colorsys

def visualize_predictions(model, dataset, class_names, num_samples=3, device='cuda'):
    model.eval()  # set model to evaluation mode
    # --- Create distinct color map for all classes ---
    N = len(class_names)
    colors = []
    for j in range(N):
        # generate color j by evenly spaced hue on [0,1)
        h = j / N
        l, s = 0.6, 0.9  # lightness and saturation for all colors
        rgb = colorsys.hls_to_rgb(h, l, s)
        colors.append(rgb)
    custom_cmap = ListedColormap(colors)

    # --- Display results for a few samples ---
    for i in range(num_samples):
        image, true_mask = dataset[i]
        # Run model prediction
        with torch.no_grad():
            input_tensor = image.unsqueeze(0).to(device)
            output = model(input_tensor)
            # If model outputs a structure with logits (e.g. from HuggingFace), handle that:
            logits = output.logits if hasattr(output, 'logits') else output
            pred_mask = torch.argmax(logits.squeeze(0), dim=0).cpu().numpy()
        # Prepare numpy image for plotting
        img_np = image.permute(1, 2, 0).cpu().numpy()

        # Plot four panels
        fig, axes = plt.subplots(1, 4, figsize=(20, 5))
        # (a) Input image
        axes[0].imshow(img_np)
        axes[0].set_title("Input Image")
        axes[0].axis('off')
        # (b) Ground truth mask
        axes[1].imshow(true_mask.numpy(), cmap=custom_cmap)
        axes[1].set_title("Ground Truth")
        axes[1].axis('off')
        # (c) Prediction mask
        axes[2].imshow(pred_mask, cmap=custom_cmap)
        axes[2].set_title("Prediction Mask")
        axes[2].axis('off')
        # (d) Overlay of prediction on input
        axes[3].imshow(img_np)
        axes[3].imshow(pred_mask, cmap=custom_cmap, alpha=0.5)
        axes[3].set_title("Overlay on Input")
        axes[3].axis('off')

        # Optional: add a legend mapping class names to colors (for reference)
        handles = [mpatches.Patch(color=colors[j], label=class_names[j]) for j in range(N)]
        fig.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.5, 1.05),
                   ncol=6, fontsize='small', frameon=False)
        plt.tight_layout()
        plt.show()

mapping = create_image_label_mapping(images_root, labels_root, max_samples=750)
num_classes = detect_num_classes(mapping)
dataset = ImageLabelDataset(mapping)


from torch.utils.data import random_split, DataLoader

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)


model = ResNet18Segmentation(num_classes)
train_losses, train_accs, val_losses, val_accs = train_model(model, train_loader, val_loader, epochs=100)
plot_training_and_validation(train_losses, train_accs, val_losses, val_accs)
visualize_predictions(model, dataset, class_names=class_ids, num_samples=3, device=device)

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

def grad_cam_segmentation(model, image_tensor, target_class, device='cuda'):
    model.eval()
    x = image_tensor.unsqueeze(0).to(device)  # [1,3,H,W]

    # 1) Hook to grab activations
    activations = {}
    def forward_hook(module, inp, out):
        activations['feat'] = out
    handle = model.encoder[-1].register_forward_hook(forward_hook)

    # 2) Forward pass
    outputs = model(x)
    logits  = outputs.logits if hasattr(outputs, 'logits') else outputs
    pred_mask = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()

    # 3) Build score for target class
    mask = (pred_mask == target_class).astype(np.float32)          # [H,W]
    mask_tensor = torch.from_numpy(mask).to(device).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]
    class_logits = logits[:, target_class:target_class+1, :, :]    # [1,1,H,W]
    score = (class_logits * mask_tensor).sum()

    # 4) Compute gradients w.r.t. activations
    grads = torch.autograd.grad(score, activations['feat'], retain_graph=False)[0]  # [1,C,h,w]
    handle.remove()

    # 5) Pool gradients and weight activations
    pooled_grads = grads.mean(dim=[0,2,3])  # [C]
    feat = activations['feat'][0]          # [C,h,w]
    for i, w in enumerate(pooled_grads):
        feat[i] *= w

    # 6) Build CAM
    cam = feat.sum(dim=0)                  # [h,w]
    cam = F.relu(cam)
    cam = cam - cam.min()
    cam = cam / (cam.max() + 1e-6)

    # 7) Detach before converting to numpy
    cam_np = cam.detach().cpu().numpy()

    return cam_np, pred_mask

def show_gradcam_overlay(image_tensor, cam, pred_mask, class_id):
    """
    image_tensor: [3,H,W] CPU tensor
    cam: [h,w] numpy array
    pred_mask: [H,W] numpy array
    """
    img_np = image_tensor.permute(1,2,0).cpu().numpy()

    # Upsample CAM to image size
    cam_img = Image.fromarray((cam * 255).astype(np.uint8)).resize(
        (img_np.shape[1], img_np.shape[0]), Image.BILINEAR
    )
    cam_np = np.array(cam_img) / 255.0

    fig, axs = plt.subplots(1,3,figsize=(15,5))
    axs[0].imshow(img_np)
    axs[0].set_title("Input Image")
    axs[0].axis("off")

    axs[1].imshow(img_np)
    axs[1].imshow(cam_np, cmap="jet", alpha=0.4)
    axs[1].set_title(f"Grad-CAM (class {class_id})")
    axs[1].axis("off")

    axs[2].imshow(pred_mask, cmap="tab20")
    axs[2].set_title("Model Prediction")
    axs[2].axis("off")

    plt.tight_layout()
    plt.show()

# get a sample
image_tensor, _ = dataset[1]             # your ImageLabelDataset
target_class    = 44                  # class ID to visualize

# compute Grad-CAM
cam, pred_mask = grad_cam_segmentation(
    model=model,
    image_tensor=image_tensor,
    target_class=target_class,
    device=device
)

# display
show_gradcam_overlay(
    image_tensor=image_tensor,
    cam=cam,
    pred_mask=pred_mask,
    class_id=target_class
)

import math

def visualize_all_gradcams(model, dataset, index=0, device='cuda'):
    """
    For the sample at `dataset[index]`, compute and display Grad-CAM heatmaps
    for every class ID present in its ground-truth label mask.
    """
    # 1) grab the sample
    image_tensor, label = dataset[index]            # image_tensor: [3,H,W], label: [H,W]
    image_np = image_tensor.permute(1,2,0).cpu().numpy()

    # 2) find all class IDs in this mask
    class_ids = np.unique(label.cpu().numpy()).tolist()
    n = len(class_ids)

    # 3) prepare grid
    cols = min(4, n)
    rows = math.ceil(n/cols)
    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))
    axes = axes.flatten()

    # 4) compute & plot each
    for ax, cls in zip(axes, class_ids):
        cam, pred_mask = grad_cam_segmentation(
            model=model,
            image_tensor=image_tensor,
            target_class=int(cls),
            device=device
        )
        # upsample cam to image size
        cam_img = Image.fromarray((cam*255).astype(np.uint8)).resize(
            (image_np.shape[1], image_np.shape[0]), Image.BILINEAR
        )
        heat_np = np.array(cam_img)/255.0

        ax.imshow(image_np)
        ax.imshow(heat_np, cmap='jet', alpha=0.4)
        ax.set_title(f"Class {cls}")
        ax.axis('off')

    # 5) turn off any extra axes
    for ax in axes[n:]:
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# Make sure grad_cam_segmentation() is defined as before
# and `dataset` is your ImageLabelDataset or similar.

visualize_all_gradcams(
    model=model,
    dataset=dataset,
    index=2,       # which sample to visualize
    device=device  # "cuda" or "cpu"
)

import matplotlib.pyplot as plt
import numpy as np

def compare_prediction_vs_truth(image_tensor, cam, pred_mask, true_label, class_id):
    """
    image_tensor: [3,H,W] CPU torch tensor
    cam:         [h,w] numpy array heatmap in [0,1]
    pred_mask:   [H,W] numpy array of predicted class IDs
    true_label:  [H,W] torch tensor of ground-truth class IDs
    class_id:    int, the class we’re inspecting
    """
    img_np = image_tensor.permute(1,2,0).cpu().numpy()
    H, W = img_np.shape[:2]

    # Resize cam → full image
    from PIL import Image
    cam_img = Image.fromarray((cam*255).astype(np.uint8)).resize((W,H), Image.BILINEAR)
    cam_np  = np.array(cam_img)/255.0

    # Build binary masks for class_id
    pred_bin = (pred_mask == class_id)
    true_bin = (true_label.cpu().numpy() == class_id)

    fig, axes = plt.subplots(1,4,figsize=(16,4))
    axes[0].imshow(img_np);        axes[0].set_title("Image");       axes[0].axis('off')
    axes[1].imshow(img_np); axes[1].imshow(cam_np, cmap='jet', alpha=0.4)
    axes[1].set_title(f"Grad-CAM for class {class_id}"); axes[1].axis('off')
    axes[2].imshow(pred_bin, cmap='gray'); axes[2].set_title("Predicted Mask"); axes[2].axis('off')
    axes[3].imshow(true_bin, cmap='gray'); axes[3].set_title("Ground-Truth Mask"); axes[3].axis('off')
    plt.tight_layout()
    plt.show()

# pick a sample and class
idx = 0
class_id = 43

# 1) Grab the sample properly
image_tensor, true_label = dataset[idx]

# 2) Call without unpacking the label
cam, pred_mask = grad_cam_segmentation(
    model=model,
    image_tensor=image_tensor,
    target_class=class_id,
    device=device
)
compare_prediction_vs_truth(
    image_tensor=image_tensor,
    cam=cam,
    pred_mask=pred_mask,
    true_label=true_label,
    class_id=class_id
)