# Neuro-240-Project
# Semantic Segmentation with CNNs and Vision Transformers on the Human Visual Diet Dataset

This project compares the performance and interpretability of three deep learning models — ResNet-18, DenseNet-121, and SegFormer (a Vision Transformer) — for semantic segmentation on the Human Visual Diet dataset, a benchmark designed to reflect human-like visual experience.

## 🧠 Motivation

While CNNs have driven significant progress in vision tasks, they often rely on superficial features and struggle with generalization. This project investigates:

- Can architectures like Vision Transformers perform better on complex, context-rich scenes?
- Do Grad-CAM (for CNNs) and attention maps (for ViTs) show human-aligned reasoning?
- What challenges do models face when trained on more "human-like" data?

## 📁 Project Structure

